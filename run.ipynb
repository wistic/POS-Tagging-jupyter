{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-Tagging\n",
    "__Project By__:\n",
    "- Katkar Prathamesh Shivaji\n",
    "- Ritesh Singh\n",
    "- Utkarsh\n",
    "\n",
    "### Brief\n",
    "Tagging is a kind of classification that may be defined as the automatic assignment of description to the tokens. Part-of-Speech (PoS) tagging may be defined as the process of assigning one of the parts of speech to the given word. \n",
    "### Problem Statement\n",
    "Develop an AI model to generate the best tag for a given wore.\n",
    "### Setup\n",
    "Let's install the requirements first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4==4.9.3 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.9.3)\r\n",
      "Requirement already satisfied: lxml==4.6.1 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (4.6.1)\r\n",
      "Requirement already satisfied: matplotlib==3.3.3 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.3.3)\r\n",
      "Requirement already satisfied: maturin==0.8.3 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.8.3)\r\n",
      "Requirement already satisfied: numpy==1.19.4 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.19.4)\r\n",
      "Requirement already satisfied: pandas==1.1.4 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.1.4)\r\n",
      "Requirement already satisfied: corpus_processor==0.1.0 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.1.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from beautifulsoup4==4.9.3->-r requirements.txt (line 1)) (2.0.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (2.4.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (2.8.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (1.3.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from matplotlib==3.3.3->-r requirements.txt (line 3)) (8.0.1)\r\n",
      "Requirement already satisfied: toml~=0.10.0 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from maturin==0.8.3->-r requirements.txt (line 4)) (0.10.2)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from pandas==1.1.4->-r requirements.txt (line 6)) (2020.4)\r\n",
      "Requirement already satisfied: six in /home/wistic/.virtualenvs/pos/lib/python3.8/site-packages (from cycler>=0.10->matplotlib==3.3.3->-r requirements.txt (line 3)) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin\n",
    "Let's see some sample data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named bs4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8143d3a249d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbs4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named bs4"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cfg import config\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "train_folder_path = config['train_folder_path']\n",
    "train_folders = os.listdir(train_folder_path)\n",
    "\n",
    "if train_folders:\n",
    "    random_folder = os.path.join(train_folder_path, train_folders[0])\n",
    "    train_files = os.listdir(random_folder)\n",
    "    if train_files:\n",
    "        random_file = os.path.join(random_folder, train_files[0])\n",
    "        with open(random_file) as f:\n",
    "            data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        count = 0\n",
    "        for tag in soup.find_all('w'):\n",
    "            count += 1\n",
    "            print(tag)\n",
    "            if count > 10:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We need to define some important variables:__\n",
    "- lowercase: Convert all data to lowercase before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase = False\n",
    "output_folder_path = config['output_folder_path']\n",
    "\n",
    "if not (os.path.exists(output_folder_path) and os.path.isdir(output_folder_path)):\n",
    "    os.mkdir(output_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to process and clean the train data before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named corpus_processor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-993974a5a907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus_processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdirWalk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_folder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_folder_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named corpus_processor"
     ]
    }
   ],
   "source": [
    "import corpus_processor\n",
    "from library import dirWalk\n",
    "\n",
    "input_folder_path = config['train_folder_path']\n",
    "\n",
    "file_list = []\n",
    "\n",
    "output_file_path = os.path.join(output_folder_path,'train-corpus_preprocessed.txt')\n",
    "file_list = dirWalk(input_folder_path,\n",
    "                    output_file_path, file_list)\n",
    "\n",
    "source_file_list = [(file_entry[0] + file_entry[1])\n",
    "                    for file_entry in file_list]\n",
    "corpus_processor.process(source_file_list, output_file_path, lowercase)\n",
    "\n",
    "print('Preprocessing complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some of the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join(output_folder_path, 'train-corpus_preprocessed.txt')\n",
    "with open(output_file) as f:\n",
    "    for i in range(10):\n",
    "        data = f.readline().strip()\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these word can occur hundred of times. Counting the frequency of the word tag combination can speed the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import clean\n",
    "\n",
    "input_file_path = os.path.join(output_folder_path, 'train-corpus_preprocessed.txt')\n",
    "with open(input_file_path, 'r') as f:\n",
    "    preprocessed_list = f.readlines()\n",
    "trimmed_list = [entry.strip('\\n') for entry in preprocessed_list]\n",
    "del preprocessed_list\n",
    "train_dictionary = dict()\n",
    "for entry in trimmed_list:\n",
    "    if entry not in train_dictionary:\n",
    "        train_dictionary[entry] = 1\n",
    "    else:\n",
    "        train_dictionary[entry] += 1\n",
    "del trimmed_list\n",
    "train_dictionary = clean(train_dictionary)\n",
    "\n",
    "print('Counting word-tag combinations complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some of the dictionary entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key, value in train_dictionary.items():\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "    print(key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the distribution of tags and words, let us find out the top 10 words and top 10 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "words = dict()\n",
    "tags = dict()\n",
    "\n",
    "for key, value in train_dictionary.items():\n",
    "    word, tag = key.split('_')\n",
    "\n",
    "    if word not in words:\n",
    "        words[word] = value\n",
    "    else:\n",
    "        words[word] = words[word] + value\n",
    "\n",
    "    if tag not in tags:\n",
    "        tags[tag] = value\n",
    "    else:\n",
    "        tags[tag] = tags[tag] + value\n",
    "        \n",
    "sorted_words = {key: value for key, value in sorted(\n",
    "    words.items(), key=lambda item: item[1], reverse=True)}\n",
    "sorted_tags = {key: value for key, value in sorted(\n",
    "    tags.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "top_words_list = [key for key in itertools.islice(sorted_words.keys(), 10)]\n",
    "top_tags_list = [key for key in itertools.islice(sorted_tags.keys(), 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 tags are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in top_tags_list:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in top_words_list:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some graphs to further understand the dataset.\n",
    "\n",
    "__Plotting Tags vs Frequency Bar Graph__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Tags vs Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Tags')\n",
    "plt.bar(range(0, len(sorted_tags) * 2, 2), list(sorted_tags.values()),\n",
    "        align='center', width=1)\n",
    "plt.xticks(range(0, len(sorted_tags) * 2, 2),\n",
    "           list(sorted_tags.keys()), rotation=90, fontsize=8)\n",
    "fig_size = plt.gcf().get_size_inches()\n",
    "plt.gcf().set_size_inches(fig_size[0] * 2, fig_size[1] * 1, forward=True)\n",
    "plt.tight_layout(pad=0.5)\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(left=-2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plotting Tags Pie Chart__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = list(itertools.islice(sorted_tags.keys(), 10))\n",
    "labels.append('Other')\n",
    "sizes = list(itertools.islice(sorted_tags.values(), 10))\n",
    "total = sum(sorted_tags.values())\n",
    "other_size = total - sum(itertools.islice(sorted_tags.values(), 10))\n",
    "sizes.append(other_size)\n",
    "\n",
    "cmap = plt.get_cmap(\"tab20c\")\n",
    "colors = cmap(np.arange(11))\n",
    "plt.pie(sizes, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plotting Words Pie Chart__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Top 10 words')\n",
    "labels = list(itertools.islice(sorted_words.keys(), 10))\n",
    "labels.append('Other')\n",
    "sizes = list(itertools.islice(sorted_words.values(), 10))\n",
    "total = sum(sorted_words.values())\n",
    "other_size = total - sum(itertools.islice(sorted_words.values(), 10))\n",
    "sizes.append(other_size)\n",
    "explode = (0, 0, 0, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0)\n",
    "cmap = plt.get_cmap(\"tab20c\")\n",
    "colors = cmap(np.arange(11) * 2)\n",
    "plt.pie(sizes, labels=labels, colors=colors, explode=explode,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup resources (Don't cleanup unless you don't want to draw above graphs again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us process the Test data also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = config['test_folder_path']\n",
    "\n",
    "file_list = []\n",
    "\n",
    "output_file_path = os.path.join(output_folder_path,'test-corpus_preprocessed.txt')\n",
    "file_list = dirWalk(input_folder_path,\n",
    "                    output_file_path, file_list)\n",
    "\n",
    "source_file_list = [(file_entry[0] + file_entry[1])\n",
    "                    for file_entry in file_list]\n",
    "corpus_processor.process(source_file_list, output_file_path, lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = os.path.join(output_folder_path,'test-corpus_preprocessed.txt')\n",
    "\n",
    "with open(output_file_path, 'r') as f:\n",
    "    preprocessed_list = f.readlines()\n",
    "trimmed_list = [entry.strip('\\n') for entry in preprocessed_list]\n",
    "del preprocessed_list\n",
    "test_dictionary = dict()\n",
    "for entry in trimmed_list:\n",
    "    if entry not in test_dictionary:\n",
    "        test_dictionary[entry] = 1\n",
    "    else:\n",
    "        test_dictionary[entry] += 1\n",
    "del trimmed_list\n",
    "test_dictionary = clean(test_dictionary)\n",
    "\n",
    "print('Test data processed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging models\n",
    "\n",
    "- [__Naive Probablility Model__](#Naive-Probabliity-Model): Jump to Naive Model\n",
    "- [__HMM Probablility Model__](#HMM-Probabliity-Model): Jump to HMM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Probability Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_map = dict()\n",
    "\n",
    "for key, value in train_dictionary.items():\n",
    "    word, tag = key.split('_')\n",
    "\n",
    "    if word not in frequency_map:\n",
    "        frequency_map[word] = dict()\n",
    "        frequency_map[word][tag] = value\n",
    "    else:\n",
    "        if tag not in frequency_map[word]:\n",
    "            frequency_map[word][tag] = value\n",
    "        else:\n",
    "            frequency_map[word][tag] += value\n",
    "\n",
    "for tag_dictionary in frequency_map.values():\n",
    "    total = sum(tag_dictionary.values())\n",
    "    for tag in tag_dictionary.keys():\n",
    "        tag_dictionary[tag] = round(\n",
    "            tag_dictionary[tag] / total, 7)\n",
    "\n",
    "tagger_map = dict()\n",
    "for word, tag_dictionary in frequency_map.items():\n",
    "    tagger_map[word] = max(tag_dictionary, key=tag_dictionary.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = dict()\n",
    "best_tag = top_tags_list[0]\n",
    "\n",
    "for key, value in test_dictionary.items():\n",
    "    word, expected_tag = key.split('_')\n",
    "\n",
    "    if key not in tagged_words:\n",
    "        if word in tagger_map:\n",
    "            assigned_tag = tagger_map[word]\n",
    "        elif word.lower() in tagger_map:\n",
    "            assigned_tag = tagger_map[word.lower()]\n",
    "        elif word.upper() in tagger_map:\n",
    "            assigned_tag = tagger_map[word.upper()]\n",
    "        elif word.capitalize() in tagger_map:\n",
    "            assigned_tag = tagger_map[word.capitalize()]\n",
    "        else:\n",
    "            assigned_tag = best_tag\n",
    "        tag_dictionary = {\n",
    "            \"word\": word,\n",
    "            \"frequency\": value,\n",
    "            \"expected_tag\": expected_tag,\n",
    "            \"assigned_tag\": assigned_tag\n",
    "        }\n",
    "        tagged_words[key] = tag_dictionary\n",
    "    else:\n",
    "        tagged_words[key][\"frequency\"] = tagged_words[key][\"frequency\"] + value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM Probability Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM model is supposed to be here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the model\n",
    "Generating confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "confusion_matrix = dict.fromkeys(sorted_tags.keys())\n",
    "for key in confusion_matrix.keys():\n",
    "    confusion_matrix[key] = dict.fromkeys(sorted_tags.keys(), 0)\n",
    "\n",
    "for value in tagged_words.values():\n",
    "    actual_tag = value['expected_tag']\n",
    "    predicted_tag = value['assigned_tag']\n",
    "    frequency = value['frequency']\n",
    "    if predicted_tag in confusion_matrix:\n",
    "        confusion_matrix[actual_tag][predicted_tag] += frequency\n",
    "\n",
    "dataframe = pd.DataFrame(confusion_matrix)\n",
    "dataframe = dataframe.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import judge\n",
    "\n",
    "average_f1_score, weighted_f1_score = judge(confusion_matrix)\n",
    "print(\"Average F1 score:\", average_f1_score)\n",
    "print(\"Weighted F1 score:\", weighted_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
